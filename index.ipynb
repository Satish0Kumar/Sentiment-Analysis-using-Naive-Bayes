import pandas as pd  # Importing the pandas library for data manipulation and analysis

# Reading the dataset from a CSV file
data = pd.read_csv("hotel_reviews.csv")
print(data)  # Display the entire dataset
data.head()  # Display the first 5 rows of the dataset
data.head(10)  # Display the first 10 rows of the dataset
data.tail()  # Display the last 5 rows of the dataset
data.tail(10)  # Display the last 10 rows of the dataset



data.isnull().sum()  # checking the null values           DATA PREPROCESSING
data.duplicated()   # checking the duplicates values


import seaborn as sns
import matplotlib.pyplot as plt

pip install wordcloud

from wordcloud import WordCloud
combined_text = " ".join(data['Rating_attribute'])   # combine all review texy into one string
wordcloud = WordCloud(width = 800, height = 400, background_color = 'white').generate(combined_text)
# plot the word cloud
plt.figure(figsize = (10,6))
plt.imshow(wordcloud,interpolation = 'bilinear')
plt.axis('off')
plt.title('Word cloud of revies')
plt.show()

from collections import Counter
targeted_words = ['good' , 'great' , 'amazing' , 'bad' ]
all_words = " ".join(data['Rating_attribute']).lower().split()  # flatter reviews into a single list of words
word_counts = Counter(all_words)
target_word_count = {word: word_counts[word] for word in targeted_words}
# plotting
plt.figure(figsize = (8,6))
plt.bar(target_word_count.keys(),target_word_count.values(), color =['blue','green','orange','black','yellow'])
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Frequency of sepicific words in review')
plt.show()


# Converting a dataset into lower case
lowercased_text = data['Rating_attribute'].str.lower()
lowercased_text
# Tokenaization
# Tokenaization is the process of breaking down a pice of text into smaller units, called tokenization
import nltk
nltk.download('punkt')

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

data['Tokens'] = data['Rating_attribute'].apply(word_tokenize)
data['Tokens']
data.info()

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stop_words = set(stopwords.words('english'))
data['Tokens'] = data['Rating_attribute'].apply(lambda x: [word for word in word_tokenize(x) if word not in stop_words])
print(data['Tokens'])


# Stemming - only taken the root words
# drive , driving , drove , diven
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
data['stemmer'] = data['Rating_attribute'].apply(lambda x: " ".join(stemmer.stem(word) for word in word_tokenize(x)))
print(data['stemmer'])
data['stemmer'].value_counts()

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
lemmatizer = WordNetLemmatizer()
data['Lemmatized'] = data['Rating_attribute'].apply(lambda x: " ".join([lemmatizer.lemmatize(word,pos=wordnet.VERB) for word in word_tokenize(x)]))
print(data['Lemmatized'])
# Remove the numbers

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
lemmatizer = WordNetLemmatizer()
data['Lemmatized'] = data['Rating_attribute'].apply(lambda x: " ".join([lemmatizer.lemmatize(word,pos=wordnet.VERB) for word in word_tokenize(x)]))
print(data['Lemmatized'])
# Remove the numbers

pip install pycontractions

pip install contractions

import contractions
data['Expanded'] = data['Rating_attribute'].apply(contractions.fix)
data['Expanded']

pip install emoji


import emoji
data['Emoji'] = data['Rating_attribute'].apply(emoji.demojize)
data['Emoji']


# Removind html tags
!pip install beautifulsoup4
from bs4 import BeautifulSoup
data['cleaned'] = data['Rating_attribute'].apply(lambda x: BeautifulSoup(x,"html.parser").get_text())
data['cleaned']
data.info()


